### List of References

1. **Logistic Regression:**
   - Agresti, A. (2018). *Statistical methods for the social sciences* (5th ed.). Pearson.
   - Hosmer, D. W., Lemeshow, S., & Sturdivant, R. X. (2013). *Applied logistic regression* (3rd ed.). Wiley.

2. **Support Vector Classifier (SVC):**
   - Cortes, C., & Vapnik, V. (1995). Support-vector networks. *Machine Learning, 20*(3), 273-297. https://doi.org/10.1007/BF00994018
   - Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

3. **Decision Tree:**
   - Breiman, L., Friedman, J., Olshen, R. A., & Stone, C. J. (1986). *Classification and regression trees*. Chapman and Hall/CRC.
   - Quinlan, J. R. (1986). Induction of decision trees. *Machine Learning, 1*(1), 81-106. https://doi.org/10.1007/BF00116251

4. **XGBoost:**
   - Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining* (pp. 785-794). ACM. https://doi.org/10.1145/2939672.2939785
   - Zhang, H., & Zhang, Y. (2016). *Introduction to XGBoost*. http://xgboost.readthedocs.io/

5. **Gradient Boosting:**
   - Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. *The Annals of Statistics, 29*(5), 1189-1232. https://doi.org/10.1214/aos/1013203451
   - Ridgeway, G. (2007). Generalized boosted regression models. In *Environmetrics: A review journal* (Vol. 15, pp. 505-515). Wiley.

6. **Artificial Neural Network (MLP):**
   - Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by backpropagating errors. *Nature, 323*(6088), 533-536. https://doi.org/10.1038/323533a0
   - Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*. MIT Press.

7. **Bagging MLP:**
   - Breiman, L. (1996). Bagging predictors. *Machine Learning, 24*(2), 123-140. https://doi.org/10.1007/BF00058655
   - Dietterich, T. G. (2000). Ensemble methods in machine learning. In *Multiple classifier systems* (pp. 1-15). Springer. https://doi.org/10.1007/3-540-45014-9_1

8. **K-Nearest Neighbors (KNN):**
   - Cover, T. M., & Hart, P. E. (1967). Nearest neighbor pattern classification. *IEEE Transactions on Information Theory, 13*(1), 21-27. https://doi.org/10.1109/TIT.1967.1053964
   - Mitchell, T. M. (1997). *Machine learning*. McGraw-Hill.

9. **Bagging KNN:**
   - Zhou, Z.-H. (2012). *Ensemble methods: Foundations and algorithms*. CRC Press.
   - Dietterich, T. G. (2000). Model selection and error estimation. In *Ensemble methods in machine learning* (Vol. 1857, pp. 1-15). Springer.

10. **Random Forest:**
    - Breiman, L. (2001). Random forests. *Machine Learning, 45*(1), 5-32. https://doi.org/10.1023/A:1010933404324
    - Liaw, A., & Wiener, M. (2002). Classification and regression by randomForest. *R news, 2*(3), 18-22.

